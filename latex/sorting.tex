\chapter{Sorting Algorithms}
\chaplabel{sorting}
\section{Merge Sort}

The #mergeSort(a)# algorithm is a classic example of recursive divide and
conquer: If the length of #a# is at most 1, then #a# is already
sorted, so we do nothing.  Otherwise, we split #a# into two halves
$#a0#=#a[0]#,\ldots,#a[n/2-1]#$ and $#a1#=#a[n/2]#,\ldots,#a[n-1]#$.
We recursively sort #a0# and #a1#, and then we merge (the now sorted)
#a0# and #a1# to get our fully sorted array #a#:
\javaimport{ods/Algorithms.mergeSort(a,c)}
Compared to sorting, merging the two sorted arrays #a0# and #a1# fairly
easy.  We add elements to #a# one at a time.  If #a0# or #a1# is empty
we add the next element from the other (non-empty) array. Otherwise,
we take the minimum of the next element in #a0# and the next element in
#a1# and add it to #a#:
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
Notice that the #merge(a0,a1,a,c)# algorithm performs at most $#n#-1$
comparisons before running out of elements in one of #a0# or #a1#.

The following theorem shows that #mergeSort(a,c)# is a very efficient algorithm:
\begin{thm}
  The #mergeSort(a,c)# algorithm runs in $O(#n#\log #n#)$ time and
  performs at most $#n#\log #n#$ comparisons.
\end{thm}

\begin{proof}
The proof is by induction on $#n#$.  The base case, in which $#n#=1$,
is trivial.

Merging two sorted lists of total length $#n#$ requires at most $#n#-1$
comparisons. If $#n#$ is even, then we apply the inductive hypothesis to
the two subproblems and obtain
\begin{align*}
  C(#n#) 
  &= #n#-1 + 2C(#n#/2) \\
  &= #n#-1 + 2((#n#/2)\log(#n#/2) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n# \enspace .
\end{align*}
The case where $#n#$ is odd is slightly more complicated.  For this case,
we use the inequality (justified below) that shows $\log (x+1/2) <
\log x + (\log e)/2x$.  So we have
\begin{align*}
  C(#n#) 
  &= #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &= #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2+1/2)\log (#n#/2+1/2) 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &< #n#-1 + \lceil #n#/2 \rceil(\log #n#/2 + (\log e)/#n#)
           + \lfloor #n#/2 \rfloor\log (#n#/2) \\
  &= #n#\log #n# - 1 + (#n#/2+1/2)(\log e/#n#) \\
  &< #n#\log #n# 
\end{align*}
since $(#n#/2+1/2)(\log e/#n#) < 1$ for all $#n#\ge 2$.

All that remains is to justify the inequality
$\log (x+1/2) <
\log x + (\log e)/2x$.  This inequality comes from the identity $\ln x = \int_1^x (1/x),\mathrm{d}x$.  This identity yields:
\[
   \ln(x+1/2) = \int_1^{x+1/2} (1/x)\,\mathrm{d}x
    = \int_1^{x} (1/x)\,\mathrm{d}x + \int_{x}^{x+1/2} (1/x)\,\mathrm{d}x
    < \int_1^{x} (1/x)\,\mathrm{d}x + 1/2x \enspace .\footnote{The same derivation yields the famous inequality, seen in many calculus books, $1+x < e^x$.}
\]
Multiplying both sides of this inequality by $\log e$ yields
$\log (x+1/2) < \log x + (\log e)/2x$.
\end{proof}

\section{Quicksort}

The \emph{quicksort} algorithm is another classic divide and conquer
algorithm.  Unlike mergesort, which does merging after solving the two
subproblems, quicksort does all its work upfront.

The algorithm is simple to describe:  Pick a random element #x# from #a#;
partition #a# into the set of elements less than #x#, the set of
elements equal to #x#, and the set of elements greater than #x#; and,
finally, recursively sort the first and third sets in this partition.
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
All of this is done in-place, so that instead of making copies of
subarrays being sorted, the #quickSort(a,i,n,c)# method only sorts the
subarray $#a[i]#,\ldots,#a[i+n-1]#$.  Initially, this method is called
as #quickSort(a,0,a.length,c)#.

At the heart of the quicksort algorithm is the in-place partitioning done that, without any extra space swaps elements in #a# and computes indices #p# and #q# so that
\[
   #a[i]# \begin{cases} 
         {}< #x# & \text{if $0\le #i#\le #p#$} \\
         {}= #x# & \text{if $#p#< #i# < #q#$} \\
         {}> #x# & \text{if $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
This partitioning, which is done by the #while# loop in the code, works
by iteratively increasing #p# and decreasing #q# while maintaining the
first and last of these conditions.  At each step, element at position
#j# is either moved to the front, left where it is, added to the back.
In the first two cases, #j# is incremented, while in the last case, #j#
is not incremented since the new element at position #j# has not been
processed yet.

The quicksort algorithms is very closely related to the random binary
search trees from \secref{rbst}.  In fact, if the input to quicksort
consists of #n# distinct elements, then the quicksort recursion tree is
a random binary search tree.

To see this, recall that when constructing a random binary search
tree the first thing we do is pick a random element #x# and make it
the root of the tree.  After this, every element will eventually be
compared to #x#, with smaller elements going into the left subtree and
larger elements going into the right subtree.  In quicksort, we select
a random element #x# and immediately compare everything to #x#, putting
the smaller elements at the beginning of the array and larger elements
at the end of the array.  Quicksort then recursively sorts the beginning
of the array and the end of the array, while the random binary search
tree recursively inserts smaller elements in the left subtree of the
root and larger elements in the right subtree of the root.

The above correspondence between random binary search trees and quicksort
means that we can translate \lemref{rbs} to a statement about quicksort:

\begin{lem}
  When quicksort is called to sort an array containing the integers
  $1,\ldots,n$, the expected number of times element #i# is compared to
  a pivot element is at most $H_{#i#} + H_{#n#-#i#}$.
\end{lem}

A little summing of harmonic numbers gives us the following theorem
about the running-time of quicksort:

\begin{thm}
  When quicksort is called to sort an array containing #n# distinct elements,
  the expected number of comparisons performed is $2\ln n - O(n)$.
\end{thm}

\begin{proof}

\end{proof}








\section{Heapsort}


